{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e582c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "# https://scikit-learn.org/stable/tutorial/basic/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5b81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa17b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [8]\n",
      "actual is: [8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "clf.fit(digits.data[:-1], digits.target[:-1])\n",
    "print('prediction:',clf.predict(digits.data[-1:]))\n",
    "print('actual is:', digits.target[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11bf0879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "iris = datasets.load_iris()\n",
    "clf = SVC()\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "list(clf.predict(iris.data[:3]))\n",
    "\n",
    "clf.fit(iris.data, iris.target_names[iris.target])\n",
    "\n",
    "list(clf.predict(iris.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05aba3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters of an estimator can be updated after it has been constructed via the set_params() method. \n",
    "# Calling fit() more than once will overwrite what was learned by any previous fit().\n",
    "# kernel was changed below from linear to rbf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "clf = SVC()\n",
    "clf.set_params(kernel='linear').fit(X, y)\n",
    "clf.predict(X[:5])\n",
    "\n",
    "clf.set_params(kernel='rbf').fit(X, y)\n",
    "clf.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7693c352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the below case, the classifier is fit on a 1d array of multiclass labels and the predict() method therefore \n",
    "# provides corresponding multiclass predictions. It is also possible to fit upon a 2d array of binary label indicators:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "classif.fit(X, y).predict(X)\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)   # changes y to a one-hot format and returns a one-hot prediction\n",
    "y\n",
    "classif.fit(X, y).predict(X)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer   #also changes y to one-hot, but for multiclass\n",
    "y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]\n",
    "y = MultiLabelBinarizer().fit_transform(y)\n",
    "y\n",
    "classif.fit(X, y).predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae7dab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "digits.images.shape    #dataset is made up of 1797 data points comprised of 8x8 images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(digits.images[-1],\n",
    "#            cmap=plt.cm.gray_r)\n",
    "\n",
    "# To use this dataset with scikit-learn, we transform each 8x8 image into a feature vector of length 64\n",
    "data = digits.images.reshape(\n",
    "    (digits.images.shape[0], -1))\n",
    "len(data[0])    #data is transoformed from 8x8 to len=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5731dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data: the main API implemented by scikit-learn is that of the estimator. An estimator is any object\n",
    "# that learns from data; it may be a classification, regression or clustering algorithm or a transformer that \n",
    "# extracts/filters useful features from raw data.\n",
    "\n",
    "\n",
    "# Estimated parameters: When data is fitted with an estimator, parameters are estimated from the data at hand.\n",
    "# All the estimated parameters are attributes of the estimator object ending by an underscore:\n",
    "# estimator.estimated_param_\n",
    "\n",
    "# Read further: Curse of dimensionality (to understand the math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68999e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total columns: 10\n",
      "with 10 coef: [ -14.6944181  -245.02259156  613.91038683  310.47859338 -716.97966491\n",
      "  395.61349832  122.26412367  217.58392863  690.2289183    62.65347874]\n",
      "mse: 2812.16321767405\n",
      "model score: 0.4384848789167006\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression for diabetes dataset\n",
    "diab_X, diab_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(diab_X, diab_y, test_size=0.33, random_state=1)\n",
    "print('total columns:',len(X_train[0]))\n",
    "\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print('with 10 coef:', regr.coef_)                                                    \n",
    "# print('mse:', \n",
    "print('mse:',np.mean((regr.predict(X_test) - y_test)**2))\n",
    "print('model score:', regr.score(X_test,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0df44507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((296, 10), (296,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e313a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43845546472823627, 0.4383659682505855, 0.4380149862766832, 0.43714243188766766, 0.4374702770785275, 0.441746264019781]\n",
      "best alpha: 0.0001\n",
      "best alpha score: 0.5429329218970074\n",
      "coef: [ -14.61216233 -244.92011502  613.96586294  310.36044645 -710.00602343\n",
      "  390.3146455   118.89922763  216.32409848  687.70073189   62.67232661]\n"
     ]
    }
   ],
   "source": [
    "# use ridge regression A solution in high-dimensional statistical learning is to shrink the regression coefficients to zero\n",
    "regr = linear_model.Ridge(alpha=.1)\n",
    "alphas = np.logspace(-4, -1, 6)\n",
    "\n",
    "print([regr.set_params(alpha=alpha)\n",
    "           .fit(X_train, y_train)\n",
    "           .score(X_test, y_test)\n",
    "       for alpha in alphas])\n",
    "\n",
    "# Capturing in the fitted parameters noise that prevents the model to generalize to new data \n",
    "# is called overfitting. The bias introduced by the ridge regression is called a regularization.\n",
    "\n",
    "# To improve the conditioning of the problem (i.e. mitigating the The curse of dimensionality), \n",
    "# it would be interesting to select only the informative features and set non-informative ones,\n",
    "# like feature 2 to 0. Ridge regression will decrease their contribution, but not set them to \n",
    "# zero. Another penalization approach, called Lasso (least absolute shrinkage and selection \n",
    "# operator), can set some coefficients to zero. Such methods are called sparse method and \n",
    "# sparsity can be seen as an application of Occamâ€™s razor: prefer simpler models.\n",
    "\n",
    "regr = linear_model.Lasso()\n",
    "scores = [regr.set_params(alpha=alpha)\n",
    "              .fit(X_train, y_train)\n",
    "              .score(X_test, y_test)\n",
    "          for alpha in alphas]\n",
    "\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "print('best alpha:', best_alpha)\n",
    "\n",
    "regr.alpha = best_alpha\n",
    "print('best alpha score:', regr.fit(X_train, y_train).score(X_train, y_train))\n",
    "print('coef:', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbb939b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Score: 0.5429332332901122\n",
      "RR Score: [0.4384819574515103, 0.43818425470247124, 0.441746264019781, 0.13491062829351286, -0.014451015318764249, -0.016541577423331066]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "ln = linear_model.LinearRegression()\n",
    "print('LR Score:',ln.fit(X_train, y_train).score(X_train, y_train))\n",
    "\n",
    "# Ridge Regression\n",
    "rr = linear_model.Ridge()\n",
    "alphas = np.logspace(-5, 5, 6)\n",
    "\n",
    "print('RR Score:', [rr.set_params(alpha=alpha)\n",
    "           .fit(X_train, y_train)\n",
    "           .score(X_test, y_test)\n",
    "       for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dd9b4cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x257c737b670>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "alphas = np.logspace(-10z, 5, 10)\n",
    "\n",
    "scr = [rr.set_params(alpha=alpha)\n",
    "           .fit(X_train, y_train)\n",
    "           .score(X_test, y_test)\n",
    "       for alpha in alphas]\n",
    "\n",
    "plt.plot(scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59dd459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x257c768ac70>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# If you have several classes to predict, an option often used is to fit one-versus-all \n",
    "# classifiers and then use a voting heuristic for the final decision.\n",
    "\n",
    "# The C parameter controls the amount of regularization in the LogisticRegression object: a \n",
    "# large value for C results in less regularization. penalty=\"l2\" gives Shrinkage \n",
    "# (i.e. non-sparse coefficients), while penalty=\"l1\" gives Sparsity.\n",
    "\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "log_score = []\n",
    "for i in range(1000,1100):\n",
    "    log = linear_model.LogisticRegression(max_iter=i, penalty='l2')\n",
    "    log_score.append(log.fit(iris_X, iris_y).score(iris_X,iris_y))\n",
    "\n",
    "plt.plot(log_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ae8e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn: 0.961111\n",
      "logreg: 0.933333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iya\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Try classifying the digits dataset with nearest neighbors and a linear model. \n",
    "# Leave out the last 10% and test prediction performance on these observations.\n",
    "\n",
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "X_digits = X_digits / X_digits.max()\n",
    "\n",
    "# get 90% of data\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[: int(0.9 * n_samples)]\n",
    "y_train = y_digits[: int(0.9 * n_samples)]\n",
    "X_test = X_digits[int(0.9 * n_samples):]\n",
    "y_test = y_digits[int(0.9 * n_samples):]\n",
    "\n",
    "# models\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# scores\n",
    "print('knn: %f' %knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('logreg: %f' %logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9f8e8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC linear:0.993333\n",
      "SVC poly:0.973333\n",
      "SVC rbf:0.973333\n"
     ]
    }
   ],
   "source": [
    "# SVMs\n",
    "# Support Vector Machines belong to the discriminant model family: they try to find a combination \n",
    "# of samples to build a plane maximizing the margin between the two classes. Regularization is set \n",
    "# by the C parameter: a small value for C means the margin is calculated using many or all of the \n",
    "# observations around the separating line (more regularization); a large value for C means the margin \n",
    "# is calculated on observations close to the separating line (less regularization).\n",
    "        \n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "print('SVC linear:%f' %svc.fit(iris_X, iris_y).score(iris_X,iris_y))\n",
    "print('SVC poly:%f' %svc.set_params(kernel='poly', degree=3).fit(iris_X, iris_y).score(iris_X, iris_y))\n",
    "print('SVC rbf:%f' %svc.set_params(kernel='rbf').fit(iris_X, iris_y).score(iris_X, iris_y))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try classifying classes 1 and 2 from the iris dataset with SVMs, with the 2 first \n",
    "# features. Leave out 10% of each class and test prediction performance on these observations.\n",
    "# Warning: the classes are ordered, do not leave out the last 10%, you would be testing on only one class.\n",
    "# Hint: You can use the decision_function method on a grid to get intuitions.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(float)\n",
    "\n",
    "X_train = X[: int(0.9 * n_sample)]\n",
    "y_train = y[: int(0.9 * n_sample)]\n",
    "X_test = X[int(0.9 * n_sample) :]\n",
    "y_test = y[int(0.9 * n_sample) :]\n",
    "\n",
    "# fit the model\n",
    "for kernel in (\"linear\", \"rbf\", \"poly\"):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor=\"k\", s=20\n",
    "    )\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], s=80, facecolors=\"none\", zorder=10, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.contour(\n",
    "        XX,\n",
    "        YY,\n",
    "        Z,\n",
    "        colors=[\"k\", \"k\", \"k\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "        levels=[-0.5, 0, 0.5],\n",
    "    )\n",
    "\n",
    "    plt.title(kernel)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "36deea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[-0.11190585 -0.04007949  0.22864503  0.60925205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iya\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "ln = linear_model.LinearRegression()\n",
    "ln.fit(iris_X, iris_y)\n",
    "\n",
    "my_coef = ln.coef_\n",
    "iris_df = pd.DataFrame(iris_X)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.rename(columns={0:'a',1:'b',2:'c',3:'d'}, inplace=True)\n",
    "\n",
    "iris_df['predict'] = iris_df.apply(lambda x:  int(x['a']*my_coef[0]) + (x['b']*my_coef[1]) + (x['c']*my_coef[2]) + (x['d']*my_coef[3]), axis=1)\n",
    "iris_df['predict2'] = 0\n",
    "iris_df['predict2'].loc[(iris_df['predict'] >= 1) & (iris_df['predict'] <=2)] = 2\n",
    "iris_df['predict2'].loc[(iris_df['predict'] >2)] = 3\n",
    "iris_df['predict2'].value_counts()\n",
    "print('Coefficients:')\n",
    "print(my_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9656fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 3  4  5  6  7  8  9 10] | Test: [0 1 2]\n",
      "Train: [ 0  1  2  5  6  7  8  9 10] | Test: [3 4]\n",
      "Train: [ 0  1  2  3  4  7  8  9 10] | Test: [5 6]\n",
      "Train: [ 0  1  2  3  4  5  6  9 10] | Test: [7 8]\n",
      "Train: [0 1 2 3 4 5 6 7 8] | Test: [ 9 10]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = ['a','a','a','b','b','c','c','c','c','c','c']\n",
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    print('Train: %s | Test: %s' % (train_indices, test_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "4221229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0 2 4 5 6 8] | Test: [1 3 7 9]\n",
      "Train: [1 3 5 6 7 8 9] | Test: [0 2 4]\n",
      "Train: [0 1 2 3 4 7 9] | Test: [5 6 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.989983305509182, 0.993322203672788, 0.9833055091819699]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9,10]\n",
    "df = pd.DataFrame(x)\n",
    "df.columns=['x']\n",
    "\n",
    "k_fold = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for my_train, my_test in k_fold.split(df):\n",
    "    print('Train: %s | Test: %s' % (my_train, my_test))\n",
    "    \n",
    "\n",
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train, test in k_fold.split(X_digits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "b9fad8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold Average Cross Val Score: 0.9279937072437099\n",
      "ssplit Average Cross Val Score: 0.9279937072437099\n",
      "gkfold Average Cross Val Score: 0.9279937072437099\n",
      "gsplit Average Cross Val Score: 0.9279937072437099\n",
      "skfold Average Cross Val Score: 0.9279937072437099\n",
      "Test Score: 0.9137748689170466\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, ShuffleSplit, GroupKFold, GroupShuffleSplit\n",
    "\n",
    "iris_X, iris_Y = datasets.load_iris(return_X_y=True)\n",
    "df = pd.DataFrame(iris_X)\n",
    "df['target'] = pd.DataFrame(iris_y)\n",
    "df.rename(columns={0:'a',1:'b',2:'c',3:'d'}, inplace=True)\n",
    "\n",
    "df_X = df.loc[:,df.columns != 'target']\n",
    "df_y = df[['target']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.33, random_state=1)\n",
    "\n",
    "ln = linear_model.LinearRegression()\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "ssplit = ShuffleSplit(n_splits=5, random_state=1)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "my_fold = ['kfold','ssplit','gkfold','gsplit','skfold']\n",
    "\n",
    "model = ln.fit(X_train, y_train)\n",
    "\n",
    "for i in range(len(my_fold)):\n",
    "    print(my_fold[i], 'Average Cross Val Score: %s' %cross_val_score(model, X_train, y_train, cv=skfold).mean())\n",
    "print('Test Score: %s' %model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "578101c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test commit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74481337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77475b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489fe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c2687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb1760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
